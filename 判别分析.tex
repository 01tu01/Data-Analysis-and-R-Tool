\section{判别分析}
    \begin{enumerate}
        \item
        \code
\begin{lstlisting}
x1 <- read.table("ex8_1-data.txt", header=T);
x <- x1[,3:4];
G <- x1[,2];
source("distance.distinguish.R")
distance.distinguish(x, G)
\end{lstlisting}
        \out
\begin{lstlisting}
> distance.distinguish(x, G)
   G   distance blong
1  1 0.03013383     1
2  1 0.50815510     1
3  1 0.33524999     1
4  1 1.34555129     1
5  1 0.43868072     1
6  1 0.66384649     2
7  1 4.37422412     1
8  1 2.84346730     1
9  1 0.34564661     1
10 1 3.48167679     1
11 1 1.37803514     1
12 1 4.28252864     1
13 2 0.26495655     2
14 2 1.39664196     1
15 2 1.25568174     2
16 2 0.63012488     2
17 2 0.61739940     2
18 2 0.43868072     1
19 2 3.90040897     2
20 2 2.06443438     2
21 2 2.50173811     2
22 2 2.05946834     2
\end{lstlisting}
        \summary\\
        根据结果，6、14、18被误判，误判率$\displaystyle \hat{p}=\frac{3}{22} \approx 13.64\%$。
        \item
        \code
\begin{lstlisting}
library(MASS)
x <- data.matrix(iris)
colnames(x) <- c("X1", "X2", "X3", "X4", "C")
x <- data.frame(x)
discrim <- lda(C~X1+X2+X3+X4, x); discrim
p.discrim <- predict(discrim)
cbind(x[[5]], p.discrim$x, p.discrim$class)
table(x[[5]], p.discrim$class)
prop.table(table(x[[5]], p.discrim$class))
\end{lstlisting}
        \out
\begin{lstlisting}
> discrim <- lda(C~X1+X2+X3+X4, x); discrim
Call:
lda(C ~ X1 + X2 + X3 + X4, data = x)

Prior probabilities of groups:
        1         2         3 
0.3333333 0.3333333 0.3333333 

Group means:
     X1    X2    X3    X4
1 5.006 3.428 1.462 0.246
2 5.936 2.770 4.260 1.326
3 6.588 2.974 5.552 2.026

Coefficients of linear discriminants:
          LD1         LD2
X1  0.8293776  0.02410215
X2  1.5344731  2.16452123
X3 -2.2012117 -0.93192121
X4 -2.8104603  2.83918785

Proportion of trace:
   LD1    LD2 
0.9912 0.0088 
> p.discrim <- predict(discrim)
> cbind(x[[5]], p.discrim$x, p.discrim$class)
             LD1          LD2  
1   1  8.0617998  0.300420621 1
2   1  7.1286877 -0.786660426 1
3   1  7.4898280 -0.265384488 1
4   1  6.8132006 -0.670631068 1
5   1  8.1323093  0.514462530 1
6   1  7.7019467  1.461720967 1
7   1  7.2126176  0.355836209 1
8   1  7.6052935 -0.011633838 1
9   1  6.5605516 -1.015163624 1
10  1  7.3430599 -0.947319209 1
11  1  8.3973865  0.647363392 1
12  1  7.2192969 -0.109646389 1
13  1  7.3267960 -1.072989426 1
14  1  7.5724707 -0.805464137 1
15  1  9.8498430  1.585936985 1
16  1  9.1582389  2.737596471 1
17  1  8.5824314  1.834489452 1
18  1  7.7807538  0.584339407 1
19  1  8.0783588  0.968580703 1
20  1  8.0209745  1.140503656 1
21  1  7.4968023 -0.188377220 1
22  1  7.5864812  1.207970318 1
23  1  8.6810429  0.877590154 1
24  1  6.2514036  0.439696367 1
25  1  6.5589334 -0.389222752 1
26  1  6.7713832 -0.970634453 1
27  1  6.8230803  0.463011612 1
28  1  7.9246164  0.209638715 1
29  1  7.9912902  0.086378713 1
30  1  6.8294645 -0.544960851 1
31  1  6.7589549 -0.759002759 1
32  1  7.3749525  0.565844592 1
33  1  9.1263463  1.224432671 1
34  1  9.4676820  1.825226345 1
35  1  7.0620139 -0.663400423 1
36  1  7.9587624 -0.164961722 1
37  1  8.6136720  0.403253602 1
38  1  8.3304176  0.228133530 1
39  1  6.9341201 -0.705519379 1
40  1  7.6882313 -0.009223623 1
41  1  7.9179372  0.675121313 1
42  1  5.6618807 -1.934355243 1
43  1  7.2410147 -0.272615132 1
44  1  6.4144356  1.247301306 1
45  1  6.8594438  1.051653957 1
46  1  6.7647039 -0.505151855 1
47  1  8.0818994  0.763392750 1
48  1  7.1867690 -0.360986823 1
49  1  8.3144488  0.644953177 1
50  1  7.6719674 -0.134893840 1
51  2 -1.4592755  0.028543764 2
52  2 -1.7977057  0.484385502 2
53  2 -2.4169489 -0.092784031 2
54  2 -2.2624735 -1.587252508 2
55  2 -2.5486784 -0.472204898 2
56  2 -2.4299673 -0.966132066 2
57  2 -2.4484846  0.795961954 2
58  2 -0.2226665 -1.584673183 2
59  2 -1.7502012 -0.821180130 2
60  2 -1.9584224 -0.351563753 2
61  2 -1.1937603 -2.634455704 2
62  2 -1.8589257  0.319006544 2
63  2 -1.1580939 -2.643409913 2
64  2 -2.6660572 -0.642504540 2
65  2 -0.3783672  0.086638931 2
66  2 -1.2011726  0.084437359 2
67  2 -2.7681025  0.032199536 2
68  2 -0.7768540 -1.659161847 2
69  2 -3.4980543 -1.684956162 2
70  2 -1.0904279 -1.626583496 2
71  2 -3.7158961  1.044514421 3
72  2 -0.9976104 -0.490530602 2
73  2 -3.8352593 -1.405958061 2
74  2 -2.2574125 -1.426794234 2
75  2 -1.2557133 -0.546424197 2
76  2 -1.4375576 -0.134424979 2
77  2 -2.4590614 -0.935277280 2
78  2 -3.5184849  0.160588866 2
79  2 -2.5897987 -0.174611728 2
80  2  0.3074879 -1.318871459 2
81  2 -1.1066918 -1.752253714 2
82  2 -0.6055246 -1.942980378 2
83  2 -0.8987038 -0.904940034 2
84  2 -4.4984664 -0.882749915 3
85  2 -2.9339780  0.027379106 2
86  2 -2.1036082  1.191567675 2
87  2 -2.1425821  0.088779781 2
88  2 -2.4794560 -1.940739273 2
89  2 -1.3255257 -0.162869550 2
90  2 -1.9555789 -1.154348262 2
91  2 -2.4015702 -1.594583407 2
92  2 -2.2924888 -0.332860296 2
93  2 -1.2722722 -1.214584279 2
94  2 -0.2931761 -1.798715092 2
95  2 -2.0059888 -0.905418042 2
96  2 -1.1816631 -0.537570242 2
97  2 -1.6161564 -0.470103580 2
98  2 -1.4215888 -0.551244626 2
99  2  0.4759738 -0.799905482 2
100 2 -1.5494826 -0.593363582 2
101 3 -7.8394740  2.139733449 3
102 3 -5.5074800 -0.035813989 3
103 3 -6.2920085  0.467175777 3
104 3 -5.6054563 -0.340738058 3
105 3 -6.8505600  0.829825394 3
106 3 -7.4181678 -0.173117995 3
107 3 -4.6779954 -0.499095015 3
108 3 -6.3169269 -0.968980756 3
109 3 -6.3277368 -1.383289934 3
110 3 -6.8528134  2.717589632 3
111 3 -4.4407251  1.347236918 3
112 3 -5.4500957 -0.207736942 3
113 3 -5.6603371  0.832713617 3
114 3 -5.9582372 -0.094017545 3
115 3 -6.7592628  1.600232061 3
116 3 -5.8070433  2.010198817 3
117 3 -5.0660123 -0.026273384 3
118 3 -6.6088188  1.751635872 3
119 3 -9.1714749 -0.748255067 3
120 3 -4.7645357 -2.155737197 3
121 3 -6.2728391  1.649481407 3
122 3 -5.3607119  0.646120732 3
123 3 -7.5811998 -0.980722934 3
124 3 -4.3715028 -0.121297458 3
125 3 -5.7231753  1.293275530 3
126 3 -5.2791592 -0.042458238 3
127 3 -4.0808721  0.185936572 3
128 3 -4.0770364  0.523238483 3
129 3 -6.5191040  0.296976389 3
130 3 -4.5837194 -0.856815813 3
131 3 -6.2282401 -0.712719638 3
132 3 -5.2204877  1.468195094 3
133 3 -6.8001500  0.580895175 3
134 3 -3.8151597 -0.942985932 2
135 3 -5.1074897 -2.130589999 3
136 3 -6.7967163  0.863090395 3
137 3 -6.5244960  2.445035271 3
138 3 -4.9955028  0.187768525 3
139 3 -3.9398530  0.614020389 3
140 3 -5.2038309  1.144768076 3
141 3 -6.6530868  1.805319760 3
142 3 -5.1055595  1.992182010 3
143 3 -5.5074800 -0.035813989 3
144 3 -6.7960192  1.460686950 3
145 3 -6.8473594  2.428950671 3
146 3 -5.6450035  1.677717335 3
147 3 -5.1795646 -0.363475041 3
148 3 -4.9677409  0.821140550 3
149 3 -5.8861454  2.345090513 3
150 3 -4.6831543  0.332033811 3
> table(x[[5]], p.discrim$class)
   
     1  2  3
  1 50  0  0
  2  0 48  2
  3  0  1 49
> prop.table(table(x[[5]], p.discrim$class))
   
              1           2           3
  1 0.333333333 0.000000000 0.000000000
  2 0.000000000 0.320000000 0.013333333
  3 0.000000000 0.006666667 0.326666667
\end{lstlisting}
        \summary\\
        setosa全部判定正确；versicolor共50个，48个判定正确，2个误判的来自virginica；virginica共50个，49个判定正确，1个误判的来自versicolor。
        \item
        \code
\begin{lstlisting}
# 距离判别分析
x1 <- read.table("data.exam8.1.1.txt", header=T);
x <- x1[,3:6];
G <- x1[,1];
source("distance.distinguish.R")
distance.distinguish(x, G)
# 逐步判别分析
source("step.distinguish.R")
Class <- factor(x1[,1])
X <- x1[,3:6]
step.distinguish(X, Class, 3, 2)
library(MASS)
discrim <- lda(类别~蛋白质.克.+碳水化合物.克., x1)
discrim.new <- predict(discrim, x1)$class; discrim.new
\end{lstlisting}
        \out
\begin{lstlisting}
> # 距离判别分析
> distance.distinguish(x, G)
   G   distance blong
1  1  4.3787100     2
2  1  1.0525988     1
3  1  4.8132482     1
4  1  2.8991588     1
5  1  5.1404677     1
6  1  0.6086319     1
7  1  4.9104902     1
8  2  1.1025444     2
9  2  1.2821348     2
10 2  1.3082985     2
11 2  2.0305928     2
12 2  1.2567908     2
13 2  0.7929908     3
14 2  6.2835284     4
15 2  1.0467843     2
16 2  2.1486510     2
17 2  1.4494785     2
18 2  1.9106217     3
19 2  1.5017083     3
20 2  0.5803590     2
21 2  2.2790242     2
22 2  1.6158079     2
23 2  7.5244729     2
24 2  3.7590954     1
25 2 16.3220759     2
26 2  1.1933420     2
27 3  1.5832875     2
28 3  0.6001095     2
29 3  0.7715365     2
30 3  0.9989526     3
31 3  1.9371579     2
32 3  6.0741457     3
33 3  5.0845694     3
34 3  1.3260521     2
35 4  2.8151348     2
36 4  6.4970138     4
37 4  4.1744166     4
38 4  3.4092930     4
39 4  1.2765777     4
40 4  2.3049596     4
41 4  7.1090074     4
42 4  0.6855894     4
43 4  1.7329603     4

> # 逐步判别分析
> step.distinguish(X, Class, 3, 2)
$Varible
[1] "蛋白质.克."     "碳水化合物.克."

$F.Value
[1] 10.125172  9.326243

$Enter.exclude
[1] "Enter" "Enter"

$F.out.max
[1] 2.470404

$F.in.min
[1] 9.326243

$F.out
[1] 0.000000 2.470404 0.000000 1.035299
> discrim.new
 [1] 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 4 2 2 2 2 2 2 2 2 2 2 1 4 4 4
[40] 4 4 4 4
Levels: 1 2 3 4
\end{lstlisting}
        \summary
        \begin{enumerate}[label=(\arabic*)]
            \item 距离判别分析：1、13、14、18、19、24、31、34、35被误判。
            \item 逐步判别分析：1、23、24、25、27、28、29、30、31、32、33、34、35、36被误判。
            \item 距离判别分析法在此题的情况下误判率低于逐步判别分析法。
        \end{enumerate}
        \item
        \code
\begin{lstlisting}
ex7_4 <- function(X.origin, X.predict) {
  if (length(unique(X.origin)) != 2) {
    print("Make sure there are only two categories in the data.");
    return(NULL)
  }
  if (length(X.origin) != length(X.predict)) {
    print("Make sure the two sets of data are consistent in length.");
    return(NULL)
  }
  flag <- 0;
  for (i in seq(1, length(X.origin))) {
    if (X.origin[i] != X.predict[i]) {
      flag = flag + 1;
    }
  }
  p <- flag / length(X.origin);
  return(p)
}
\end{lstlisting}
利用题1的数据进行输出
\begin{lstlisting}
x1 <- read.table("ex8_1-data.txt", header=T);
x <- x1[,3:4];
X.origin <- x1[,2];
source("distance.distinguish.R")
X.new <- distance.distinguish(x, G)
X.new <- X.new[,3]
source("ex7_4.R")
ex7_4(X.origin, X.new)
\end{lstlisting}
        \out
\begin{lstlisting}
> ex7_4(X.origin, X.new)
[1] 0.1363636
\end{lstlisting}
        \summary\\
        在题1的数据下，误判率大致为$13.64\%$。
        \item
        \code
\begin{lstlisting}
ex7_5 <- function(data, C) {
  # data是样本数据，C是样本所属的类别，使用Fisher线性判别
  # 判断是否只有2类
  if (length(unique(C)) != 2) {
    print("Make sure there are only two categories in the data.");
    return(NULL)
  }
  # 读取数据并进行基础的命名
  m <- ncol(data);
  cols <- sprintf("X%d", seq(1:m));
  x <- cbind(C, data);
  colnames(x) <- c("C", cols);
  n <- nrow(x);
  # 进行判别
  n_wrong = 0;
  for (i in seq(1:n)) {
    x_new <- rbind(x[1:i - 1,], x[i + 1:n,]);
    discrim <- lda(C ~ ., x_new);
    p.discrim <- predict(discrim, x[i,]);
    if (p.discrim$class != x[i, 1]) {
      n_wrong = n_wrong + 1;
    }
  }
  p <- n_wrong / n;
  return(p);
}
\end{lstlisting}
利用题1的数据进行输出
\begin{lstlisting}
x1 <- read.table("ex8_1-data.txt", header=T);
data <- x1[,3:4];
C <- x1[,2];
source("ex7_5.R");
ex7_5(data, C);
\end{lstlisting}
        \out
\begin{lstlisting}
> ex7_5(data, C);
[1] 0.2272727
\end{lstlisting}
        \summary\\
        在题1的数据下，误判率大致为$22.73\% > 13.64\%$，高于回代法。在样本量偏小时，交叉确认估计误判率时准确率偏低，适用于样本量更大的情况。
    \end{enumerate}
\clearpage